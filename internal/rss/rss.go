package rss

import (
    "log"
    "net/http"
    "strings"
    "sync"
    "time"
    "unicode"

    "github.com/mmcdole/gofeed"
    "rss2tg/internal/storage"
)

type MessageHandler func(title, url, group string, pubDate time.Time, matchedKeywords []string) error

type Manager struct {
    feeds          []*Feed
    db             *storage.Storage
    messageHandler MessageHandler
    mu             sync.Mutex
}

type Feed struct {
    URLs            []string
    Interval        time.Duration
    Keywords        []string
    Group           string
    AllowPartMatch  bool      // æ˜¯å¦å…è®¸éƒ¨åˆ†åŒ¹é…
    ticker          *time.Ticker
    stopChan        chan struct{}
}

type Config struct {
    URLs            []string
    Interval        int
    Keywords        []string
    Group           string
    AllowPartMatch  bool      // æ˜¯å¦å…è®¸éƒ¨åˆ†åŒ¹é…
}

func NewManager(configs []Config, db *storage.Storage) *Manager {
    manager := &Manager{
        db: db,
    }
    manager.UpdateFeeds(configs)
    return manager
}

func (m *Manager) SetMessageHandler(handler MessageHandler) {
    m.messageHandler = handler
}

func (m *Manager) UpdateFeeds(configs []Config) {
    m.mu.Lock()
    defer m.mu.Unlock()

    // åœæ­¢æ‰€æœ‰ç°æœ‰çš„feedè½®è¯¢å™¨
    for _, feed := range m.feeds {
        if feed.stopChan != nil {
            close(feed.stopChan)
        }
    }

    // åˆ›å»ºæ–°çš„feeds
    m.feeds = make([]*Feed, len(configs))
    for i, config := range configs {
        m.feeds[i] = &Feed{
            URLs:           config.URLs,
            Interval:       time.Duration(config.Interval) * time.Second,
            Keywords:       config.Keywords,
            Group:          config.Group,
            AllowPartMatch: config.AllowPartMatch,  // æ·»åŠ éƒ¨åˆ†åŒ¹é…é…ç½®
            stopChan:       make(chan struct{}),
        }
    }

    // å¯åŠ¨æ–°çš„feedè½®è¯¢å™¨
    for _, feed := range m.feeds {
        go m.pollFeed(feed)
    }
}

func (m *Manager) Start() {
    log.Println("RSSç®¡ç†å™¨å·²å¯åŠ¨")
}

func (m *Manager) pollFeed(feed *Feed) {
    feed.ticker = time.NewTicker(feed.Interval)
    defer feed.ticker.Stop()

    for {
        select {
        case <-feed.ticker.C:
            for _, url := range feed.URLs {
                log.Printf("æ£€æŸ¥feed: %s", url)
                m.checkFeed(feed, url)
            }
        case <-feed.stopChan:
            log.Printf("åœæ­¢feedè½®è¯¢å™¨: %v", feed.URLs)
            return
        }
    }
}

func (m *Manager) checkFeed(feed *Feed, url string) {
    fp := gofeed.NewParser()
    
    // åˆ›å»ºè‡ªå®šä¹‰çš„ HTTP å®¢æˆ·ç«¯
    client := &http.Client{
        Timeout: 30 * time.Second,
    }
    
    // åˆ›å»ºè‡ªå®šä¹‰çš„è¯·æ±‚
    req, err := http.NewRequest("GET", url, nil)
    if err != nil {
        log.Printf("åˆ›å»ºè¯·æ±‚å¤±è´¥ %s: %v", url, err)
        return
    }
    
    // æ·»åŠ æµè§ˆå™¨æ ‡è¯†å’Œå…¶ä»–å¿…è¦çš„å¤´ä¿¡æ¯
    req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    req.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
    req.Header.Set("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
    req.Header.Set("Connection", "keep-alive")
    req.Header.Set("Upgrade-Insecure-Requests", "1")
    
    // ä½¿ç”¨è‡ªå®šä¹‰å®¢æˆ·ç«¯è§£æ Feed
    fp.Client = client
    parsedFeed, err := fp.ParseURL(url)
    if err != nil {
        log.Printf("è§£æFeed %så¤±è´¥: %v", url, err)
        return
    }

    for _, item := range parsedFeed.Items {
        matchedKeywords := m.matchKeywords(item, feed)
        if len(matchedKeywords) > 0 {
            log.Printf("å‘ç°æ–°é¡¹ç›®: %s", item.Title)
            if err := m.messageHandler(item.Title, item.Link, feed.Group, *item.PublishedParsed, matchedKeywords); err != nil {
                log.Printf("å‘é€æ¶ˆæ¯å¤±è´¥: %v", err)
            } else {
                log.Printf("æˆåŠŸå‘é€é¡¹ç›®çš„æ¶ˆæ¯: %s", item.Title)
                m.db.MarkAsSent(item.Link)
            }
        }
    }
}

// normalizeText æ ‡å‡†åŒ–æ–‡æœ¬ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç©ºç™½
func normalizeText(text string) string {
    // 1. è½¬æ¢ä¸ºå°å†™
    text = strings.ToLower(text)
    
    // 2. æ›¿æ¢å¸¸è§çš„ç‰¹æ®Šå­—ç¬¦ç»„åˆ
    replacements := map[string]string{
        "c++": "cpp",
        "c#": "csharp",
        ".net": "dotnet",
    }
    
    for old, new := range replacements {
        text = strings.ReplaceAll(text, old, new)
    }
    
    // 3. æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼
    var result strings.Builder
    for _, ch := range text {
        if unicode.IsLetter(ch) || unicode.IsNumber(ch) || unicode.IsSpace(ch) {
            result.WriteRune(ch)
        } else {
            // ç”¨ç©ºæ ¼æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
            result.WriteRune(' ')
        }
    }
    
    // 4. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    return strings.Join(strings.Fields(result.String()), " ")
}

// isWordMatch æ£€æŸ¥å•è¯æ˜¯å¦å®Œå…¨åŒ¹é…
func isWordMatch(text, keyword string) bool {
    words := strings.Fields(text)
    for _, word := range words {
        if word == keyword {
            return true
        }
    }
    return false
}

// contains æ£€æŸ¥å­—ç¬¦ä¸²åˆ‡ç‰‡æ˜¯å¦åŒ…å«ç‰¹å®šå­—ç¬¦ä¸²
func contains(slice []string, str string) bool {
    for _, v := range slice {
        if v == str {
            return true
        }
    }
    return false
}

func (m *Manager) matchKeywords(item *gofeed.Item, feed *Feed) []string {
    if m.db.WasSent(item.Link) {
        return nil
    }

    if len(feed.Keywords) == 0 {
        return []string{"æ— å…³é”®è¯"}
    }

    // æ ‡å‡†åŒ–æ–‡æœ¬
    normalizedTitle := normalizeText(item.Title)
    normalizedDesc := normalizeText(item.Description)
    
    var matched []string
    
    // æ£€æŸ¥æ¯ä¸ªå…³é”®è¯
    for _, keyword := range feed.Keywords {
        // æ ‡å‡†åŒ–å…³é”®è¯
        normalizedKeyword := normalizeText(keyword)
        
        // é¦–å…ˆå°è¯•å®Œæ•´è¯åŒ¹é…
        if isWordMatch(normalizedTitle, normalizedKeyword) {
            if !contains(matched, keyword) {
                matched = append(matched, keyword)
            }
            continue
        }
        
        if isWordMatch(normalizedDesc, normalizedKeyword) {
            if !contains(matched, keyword) {
                matched = append(matched, keyword)
            }
            continue
        }
        
        // å¦‚æœå…è®¸éƒ¨åˆ†åŒ¹é…ä¸”æ²¡æœ‰æ‰¾åˆ°å®Œæ•´åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
        if feed.AllowPartMatch {
            if strings.Contains(normalizedTitle, normalizedKeyword) {
                if !contains(matched, keyword) {
                    matched = append(matched, keyword)
                }
            } else if strings.Contains(normalizedDesc, normalizedKeyword) {
                if !contains(matched, keyword) {
                    matched = append(matched, keyword)
                }
            }
        }
    }

    // æ ¹æ®æ˜¯å¦åŒ¹é…åˆ°å…³é”®è¯æ¥å†³å®šæ—¥å¿—è¾“å‡ºçº§åˆ«
    if len(matched) > 0 {
        // å¦‚æœåŒ¹é…åˆ°å…³é”®è¯ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—
        log.Printf("ğŸ“ å‘ç°åŒ¹é…æ–‡ç« :\n"+
            "   æ ‡é¢˜: %s\n"+
            "   æè¿°: %s\n"+
            "   é“¾æ¥: %s\n"+
            "   éƒ¨åˆ†åŒ¹é…: %s\n"+
            "âœ¨ åŒ¹é…å…³é”®è¯: %v",
            item.Title,
            item.Description,
            item.Link,
            map[bool]string{true: "å…è®¸", false: "ç¦ç”¨"}[feed.AllowPartMatch],
            matched)
    } else {
        // å¦‚æœæœªåŒ¹é…åˆ°å…³é”®è¯ï¼Œåªè¾“å‡ºç®€å•çš„ç›‘å¬çŠ¶æ€
        log.Printf("ğŸ‘€ ç›‘å¬RSS: %s, æ ‡é¢˜: %s", feed.URLs[0], item.Title)
    }

    return matched
}
